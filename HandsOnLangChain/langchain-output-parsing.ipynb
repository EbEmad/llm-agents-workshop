{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Hands-On LangChain for LLM Applications Development: Output Parsing </center>\n",
    "\n",
    "***\n",
    "\n",
    "When developing a complex application with a Language Model (LLM), it‚Äôs common to specify the desired output format, such as JSON, and designate particular keys for organizing the data. \n",
    "\n",
    "Let‚Äôs consider the chain of thought reasoning method as an illustrative example. In this method, the LLM‚Äôs thinking process is represented by distinct stages: ‚Äúthought‚Äù indicates the reasoning process, ‚Äúaction‚Äù denotes the subsequent action taken, and ‚Äúobservation‚Äù reflects the learning acquired from that action, and so forth. By crafting a prompt that directs the LLM to utilize these specific keywords (thought, action, observation), we can effectively guide its cognitive process. \n",
    "\n",
    "In this article, we will cover coupling the prompt with a parser that allows for the extraction of text associated with certain keywords from the LLM‚Äôs output. This combined approach offers a streamlined means of specifying input for the LLM and accurately interpreting its output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Parsing Output Using LangChain Prompt Templates </b></div>\n",
    "\n",
    "\n",
    "Let's start with an example to clarify the output parsing concept. Let‚Äôs take a look at how you can have an LLM output JSON, and use LangChain to parse that output. \n",
    "\n",
    "In this example, we will extract information from a product review and format that output in a JSON format. Here‚Äôs an example of how you would like the output formatted. Technically, this is a Python dictionary, where whether or not the product is a gift, maps to false, the number of days it took to deliver was five, and the price value was pretty affordable. So this is one example of a desired output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:11:23.902335Z",
     "iopub.status.busy": "2025-03-23T14:11:23.901939Z",
     "iopub.status.idle": "2025-03-23T14:11:23.907793Z",
     "shell.execute_reply": "2025-03-23T14:11:23.906583Z",
     "shell.execute_reply.started": "2025-03-23T14:11:23.902309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = {\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:11:24.366394Z",
     "iopub.status.busy": "2025-03-23T14:11:24.365291Z",
     "iopub.status.idle": "2025-03-23T14:11:24.373056Z",
     "shell.execute_reply": "2025-03-23T14:11:24.371628Z",
     "shell.execute_reply.started": "2025-03-23T14:11:24.366356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x.get(\"price_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a customer review, as well as a template and we want to get the output as JSON. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:11:26.61767Z",
     "iopub.status.busy": "2025-03-23T14:11:26.617287Z",
     "iopub.status.idle": "2025-03-23T14:11:26.623164Z",
     "shell.execute_reply": "2025-03-23T14:11:26.622107Z",
     "shell.execute_reply.started": "2025-03-23T14:11:26.617641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the review template asks the LLM to take as input a customer review extract these three fields and then format the output as JSON with the following keys. \n",
    "\n",
    "So here‚Äôs how you can wrap this in LangChain. First, we will import the chat prompt template. Then we will have the prompt templates created from the review template up on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:11:27.915409Z",
     "iopub.status.busy": "2025-03-23T14:11:27.915026Z",
     "iopub.status.idle": "2025-03-23T14:12:18.1259Z",
     "shell.execute_reply": "2025-03-23T14:12:18.12445Z",
     "shell.execute_reply.started": "2025-03-23T14:11:27.91538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:15:55.168953Z",
     "iopub.status.busy": "2025-03-23T14:15:55.168472Z",
     "iopub.status.idle": "2025-03-23T14:16:07.441035Z",
     "shell.execute_reply": "2025-03-23T14:16:07.439318Z",
     "shell.execute_reply.started": "2025-03-23T14:15:55.168915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:16:56.913829Z",
     "iopub.status.busy": "2025-03-23T14:16:56.913274Z",
     "iopub.status.idle": "2025-03-23T14:16:56.920754Z",
     "shell.execute_reply": "2025-03-23T14:16:56.919631Z",
     "shell.execute_reply.started": "2025-03-23T14:16:56.913799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs create the messages to pass to the OpenAI, endpoint. Create the OpenAI endpoint, call that endpoint, and then let‚Äôs print out the response. I encourage you to pause the video and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:30:59.539962Z",
     "iopub.status.busy": "2025-03-23T14:30:59.539396Z",
     "iopub.status.idle": "2025-03-23T14:30:59.659531Z",
     "shell.execute_reply": "2025-03-23T14:30:59.658485Z",
     "shell.execute_reply.started": "2025-03-23T14:30:59.539927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "openai.api_key=userdata.get('api_key')\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "llm_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:31:47.462593Z",
     "iopub.status.busy": "2025-03-23T14:31:47.462221Z",
     "iopub.status.idle": "2025-03-23T14:31:49.591403Z",
     "shell.execute_reply": "2025-03-23T14:31:49.590087Z",
     "shell.execute_reply.started": "2025-03-23T14:31:47.462563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=openai.api_key)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says the gift is true, the delivery day is 2, and the price value also looks pretty accurate. But note that if we check the type of the response, this is a string. So it looks like JSON and has key-value pairs, but it‚Äôs not a dictionary. This is just one long string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:33:58.357868Z",
     "iopub.status.busy": "2025-03-23T14:33:58.357414Z",
     "iopub.status.idle": "2025-03-23T14:33:58.365348Z",
     "shell.execute_reply": "2025-03-23T14:33:58.364329Z",
     "shell.execute_reply.started": "2025-03-23T14:33:58.357835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what I‚Äôd like to do is go to the response content and get the value from the gift key which should be true, but if I run this, this should generate an error because, well, this is a string. This is not a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:40:38.503837Z",
     "iopub.status.busy": "2025-03-23T14:40:38.502532Z",
     "iopub.status.idle": "2025-03-23T14:40:38.508747Z",
     "shell.execute_reply": "2025-03-23T14:40:38.507625Z",
     "shell.execute_reply.started": "2025-03-23T14:40:38.5038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "\n",
    "# response.content.get('gift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:40:40.83029Z",
     "iopub.status.busy": "2025-03-23T14:40:40.829903Z",
     "iopub.status.idle": "2025-03-23T14:40:40.837401Z",
     "shell.execute_reply": "2025-03-23T14:40:40.836261Z",
     "shell.execute_reply.started": "2025-03-23T14:40:40.830261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "response.content[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs see how we will use LangChain‚Äôs parser to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Parse the LLM Output String into a Python Dictionary </b></div>\n",
    "\n",
    "\n",
    "To parse the output into json format and make its type Python dict not string we will start with importing the response schema and structured output parser from LangChain. \n",
    "\n",
    "I am going to tell it what I wanted to parse by specifying these response schemas. So the gif schema, delivery days schema, and price value schema, here are their description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:47:44.771246Z",
     "iopub.status.busy": "2025-03-23T14:47:44.770302Z",
     "iopub.status.idle": "2025-03-23T14:47:44.841157Z",
     "shell.execute_reply": "2025-03-23T14:47:44.840186Z",
     "shell.execute_reply.started": "2025-03-23T14:47:44.771206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I‚Äôve specified the schema for these LangChain can give you the prompt itself by having the output parser tell you what instructions it wants you to send to the LLM. So if I were to print format instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:50:37.960079Z",
     "iopub.status.busy": "2025-03-23T14:50:37.959642Z",
     "iopub.status.idle": "2025-03-23T14:50:37.965667Z",
     "shell.execute_reply": "2025-03-23T14:50:37.96463Z",
     "shell.execute_reply.started": "2025-03-23T14:50:37.960046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a precise set of instructions for the LLM that will cause it to generate an output that the output parser can process. So here‚Äôs the new review template and the review template includes the format instructions that LangChain generated.\n",
    "\n",
    "We can create a prompt from the review template too, and then create the messages that will pass to the OpenAI endpoint. If you want, you can take a look at the actual prompt, which gives the instructions to extract the fields gift, delivery days, and price value, here‚Äôs the text, and then here are the formatting instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:59:55.854868Z",
     "iopub.status.busy": "2025-03-23T14:59:55.854412Z",
     "iopub.status.idle": "2025-03-23T14:59:55.86205Z",
     "shell.execute_reply": "2025-03-23T14:59:55.860667Z",
     "shell.execute_reply.started": "2025-03-23T14:59:55.854838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T14:59:56.231631Z",
     "iopub.status.busy": "2025-03-23T14:59:56.231274Z",
     "iopub.status.idle": "2025-03-23T14:59:56.239287Z",
     "shell.execute_reply": "2025-03-23T14:59:56.237829Z",
     "shell.execute_reply.started": "2025-03-23T14:59:56.231605Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we call the OpenAI endpoint, let‚Äôs take a look at what response we got. It is now this, and now if we use the output parser that we created earlier, you can then parse this into an output dictionary, which if I print, looks like this. \n",
    "\n",
    "Notice that this is of type dictionary, not a string, which is why I can now extract the value associated with the key gift and get true, or the value associated with delivery days and get two, or you can also extract the value associated with price value. So this is a nifty way to take your LLM output and parse it into a Python dictionary, to make the output easier to use in downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:11:21.370202Z",
     "iopub.status.busy": "2025-03-23T15:11:21.368986Z",
     "iopub.status.idle": "2025-03-23T15:11:21.374745Z",
     "shell.execute_reply": "2025-03-23T15:11:21.373589Z",
     "shell.execute_reply.started": "2025-03-23T15:11:21.370165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# response = chat(messages)\n",
    "# output_dict = output_parser.parse(response.content)\n",
    "# output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ‡ºº‚Å† ‚Å†„Å§‚Å† ‚Å†‚óï‚Å†‚Äø‚Å†‚óï‚Å† ‚Å†‡ºΩ‚Å†„Å§ Thank You!</b></div>\n",
    "‚Äã\n",
    "<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> üíå Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> üöÄ If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ‚ù§Ô∏è Once again, thank you for your support, and I hope to see you again soon!</p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
