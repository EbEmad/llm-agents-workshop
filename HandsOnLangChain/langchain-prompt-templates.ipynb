{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Hands-On LangChain for LLM Applications Development: Prompt Templates </center>\n\n# <center style=\"font-family: consolas; font-size: 25px; font-weight: bold;\">  Understanding LangChain Prompt Templates   </center>\n***\n\nBy prompting an LLM or large language model, it is possible to develop complex AI applications much faster than ever before. However, an application can require prompting an LLM multiple times and parsing its output, so a lot of glue code must be written.\n\nLangChain makes this development process much easier by using an easy set of abstractions to do this type of operation and by providing prompt templates. In this notebook, we will cover prompt templates, why it is important, and how to use them effectively, explained with practical examples.\n\n\n#### <a id=\"top\"></a>\n# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n\n<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n<ul>\n    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Setting Up Working Environment & Getting Started </a> </li>\n    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Prompt Template using LangChain </a></li>\n    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Why do We Need LangChain Prompt Templates? </a></li> \n</ul>\n</div>\n\n***\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Setting Up Working Environment & Getting Started </b></div>\n\n\nTo get started we are going to import OS, import OpenAI, and load my OpenAI secret key. If you‚Äôre running this locally and don‚Äôt have OpenAI installed yet, you might need to run pip to install OpenAI.","metadata":{}},{"cell_type":"code","source":"!pip install openai","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from openai import OpenAI\nimport openai\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nopenai.api_key = user_secrets.get_secret(\"openai_api\")\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=openai.api_key,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T11:51:51.351382Z","iopub.execute_input":"2024-04-14T11:51:51.351782Z","iopub.status.idle":"2024-04-14T11:51:51.52871Z","shell.execute_reply.started":"2024-04-14T11:51:51.35175Z","shell.execute_reply":"2024-04-14T11:51:51.527697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After that, we will define the LLM model that we will use which will be gpt-3.5-turbo.","metadata":{}},{"cell_type":"code","source":"llm_model = \"gpt-3.5-turbo\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T11:54:05.779477Z","iopub.execute_input":"2024-04-14T11:54:05.779936Z","iopub.status.idle":"2024-04-14T11:54:05.786875Z","shell.execute_reply.started":"2024-04-14T11:54:05.779904Z","shell.execute_reply":"2024-04-14T11:54:05.78536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we will define a helper function that will take the input prompt and return the response.","metadata":{}},{"cell_type":"code","source":"def get_completion(prompt, \n                   llm_model=llm_model, \n                   temperature=0, \n                   max_tokens=500):\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = client.chat.completions.create(\n        model=llm_model,\n        messages=messages,\n        temperature=temperature, \n        max_tokens=max_tokens, \n    )\n    return response.choices[0].message.content\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:01:57.671568Z","iopub.execute_input":"2024-04-14T12:01:57.672373Z","iopub.status.idle":"2024-04-14T12:01:57.680324Z","shell.execute_reply.started":"2024-04-14T12:01:57.672327Z","shell.execute_reply":"2024-04-14T12:01:57.679178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let‚Äôs start with an example where you get an email from a customer in a language other than formal English. To make sure the example is understandable, the other language we will use is the English pirate language:\n","metadata":{}},{"cell_type":"code","source":"customer_email = \"\"\"\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse,\\\nthe warranty don't cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:00:34.464648Z","iopub.execute_input":"2024-04-14T12:00:34.465128Z","iopub.status.idle":"2024-04-14T12:00:34.470722Z","shell.execute_reply.started":"2024-04-14T12:00:34.465091Z","shell.execute_reply":"2024-04-14T12:00:34.469766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will ask the LLM to translate the text to formal English in a calm and respectful tone. I will set the style to American English in a calm and respectful tone. I will specify the prompt using an f-string with the instructions, translate the text that is delimited by triple backticks into style, and then plug in these two styles. This generates a prompt that says translate the text.","metadata":{}},{"cell_type":"code","source":"style = \"\"\"American English \\\nin a calm and respectful tone\n\"\"\"\n\nprompt = f\"\"\"Translate the text \\\nthat is delimited by triple backticks \ninto a style that is {style}.\ntext: ```{customer_email}```\n\"\"\"\n\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:00:57.467511Z","iopub.execute_input":"2024-04-14T12:00:57.468443Z","iopub.status.idle":"2024-04-14T12:00:57.475478Z","shell.execute_reply.started":"2024-04-14T12:00:57.468402Z","shell.execute_reply":"2024-04-14T12:00:57.474347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let‚Äôs see what the response is:","metadata":{}},{"cell_type":"code","source":"response = get_completion(prompt)\nresponse","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:02:01.232552Z","iopub.execute_input":"2024-04-14T12:02:01.23315Z","iopub.status.idle":"2024-04-14T12:02:02.424924Z","shell.execute_reply.started":"2024-04-14T12:02:01.233117Z","shell.execute_reply":"2024-04-14T12:02:02.423673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"That sounds very nice and calm. Therefore if you have different customers writing reviews in different languages, not just English pirates, but French, German, Japanese, and so on, you can imagine having to generate a whole sequence of prompts to generate such translations. Let‚Äôs look at how we can do this in a more convenient, way using **LangChain**.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Prompt Template using LangChain </b></div>\n\n\n\nLet's start with importing chat OpenAI. This is LangChain‚Äôs abstraction for the chatGPT API endpoint. I will set the temperature parameter to be equal to zero to make the output a little bit less random.","metadata":{}},{"cell_type":"code","source":"!pip install pip install langchain-community","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:05:32.567155Z","iopub.execute_input":"2024-04-14T12:05:32.56757Z","iopub.status.idle":"2024-04-14T12:05:51.863586Z","shell.execute_reply.started":"2024-04-14T12:05:32.567538Z","shell.execute_reply":"2024-04-14T12:05:51.862658Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_community.chat_models import ChatOpenAI\n\n# To control the randomness and creativity of the generated\n# text by an LLM, use temperature = 0.0\n\nchat = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=openai.api_key)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:29:32.906125Z","iopub.execute_input":"2024-04-14T12:29:32.906577Z","iopub.status.idle":"2024-04-14T12:29:32.943979Z","shell.execute_reply.started":"2024-04-14T12:29:32.906541Z","shell.execute_reply":"2024-04-14T12:29:32.942832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will define the template string as follows. Translate the text delimited by triple vectors into a style that is style, and then here‚Äôs the text.","metadata":{}},{"cell_type":"code","source":"template_string = \"\"\"Translate the text \\\nthat is delimited by triple backticks \\\ninto a style that is {style}. \\\ntext: ```{text}```\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:37:38.173082Z","iopub.execute_input":"2024-04-14T12:37:38.173489Z","iopub.status.idle":"2024-04-14T12:37:38.179377Z","shell.execute_reply.started":"2024-04-14T12:37:38.173459Z","shell.execute_reply":"2024-04-14T12:37:38.178032Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To repeatedly reuse this template, we have to import LangChain‚Äôs chat prompt template, and then, let me create a prompt template using that template string that we just wrote above.","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:37:45.308162Z","iopub.execute_input":"2024-04-14T12:37:45.308614Z","iopub.status.idle":"2024-04-14T12:37:45.317049Z","shell.execute_reply.started":"2024-04-14T12:37:45.30858Z","shell.execute_reply":"2024-04-14T12:37:45.31575Z"}}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_template(template_string)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:38:41.790344Z","iopub.execute_input":"2024-04-14T12:38:41.790967Z","iopub.status.idle":"2024-04-14T12:38:41.892201Z","shell.execute_reply.started":"2024-04-14T12:38:41.790934Z","shell.execute_reply":"2024-04-14T12:38:41.89101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the prompt template, you can extract the original prompt, and it realizes that this prompt has two input variables, the style, and the text, shown here with the curly braces.","metadata":{}},{"cell_type":"code","source":"prompt_template.messages[0].prompt\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:38:58.792761Z","iopub.execute_input":"2024-04-14T12:38:58.793155Z","iopub.status.idle":"2024-04-14T12:38:58.800958Z","shell.execute_reply.started":"2024-04-14T12:38:58.793123Z","shell.execute_reply":"2024-04-14T12:38:58.799807Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also print the input variables out, and you can see that it realizes it has two input variables the style and text","metadata":{}},{"cell_type":"code","source":"prompt_template.messages[0].prompt.input_variables\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:39:20.927667Z","iopub.execute_input":"2024-04-14T12:39:20.928365Z","iopub.status.idle":"2024-04-14T12:39:20.934872Z","shell.execute_reply.started":"2024-04-14T12:39:20.928332Z","shell.execute_reply":"2024-04-14T12:39:20.933739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, let‚Äôs specify the style. This is a style that I want the customer message to be translated to, so I‚Äôm going to call this customer style, and here‚Äôs my same customer email as before.","metadata":{}},{"cell_type":"code","source":"customer_style = \"\"\"American English \\\nin a calm and respectful tone\n\"\"\"\n\ncustomer_email = \"\"\"\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse, \\\nthe warranty don't cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T12:55:42.68933Z","iopub.execute_input":"2024-04-14T12:55:42.689755Z","iopub.status.idle":"2024-04-14T12:55:42.695461Z","shell.execute_reply.started":"2024-04-14T12:55:42.689722Z","shell.execute_reply":"2024-04-14T12:55:42.694143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If I create customer messages, this will generate the prompt and will pass this large language model to get a response.\n\nIf you want to look at the types, the customer message is a list, and if you look at the first element of the list, this is more or less the prompt that you would expect this to be creating.\n","metadata":{}},{"cell_type":"code","source":"customer_messages = prompt_template.format_messages(\n                    style=customer_style,\n                    text=customer_email)\n\nprint(type(customer_messages))\nprint(type(customer_messages[0]))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:09:37.012245Z","iopub.execute_input":"2024-04-14T13:09:37.012648Z","iopub.status.idle":"2024-04-14T13:09:37.021123Z","shell.execute_reply.started":"2024-04-14T13:09:37.012618Z","shell.execute_reply":"2024-04-14T13:09:37.019406Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lastly, let‚Äôs pass this prompt to the LLM, so I‚Äôm going to call chat, which we had set earlier, as a reference to the OpenAI chatGPT endpoint, and, if we print out the customer responses content, then, it gives you back this text translated from English pirate to polite American English","metadata":{}},{"cell_type":"code","source":"# Call the LLM to translate to the style of the customer message\ncustomer_response = chat(customer_messages)\n\nprint(customer_response.content)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:09:39.651912Z","iopub.execute_input":"2024-04-14T13:09:39.652277Z","iopub.status.idle":"2024-04-14T13:09:40.753381Z","shell.execute_reply.started":"2024-04-14T13:09:39.65225Z","shell.execute_reply":"2024-04-14T13:09:40.752242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Of course, you can imagine other use cases where the customer emails are in other languages and this too can be used to translate the messages for an English-speaking to understand and reply to.\n\nSo let‚Äôs say, an English-speaking customer service agent writes this and says,","metadata":{}},{"cell_type":"code","source":"service_reply = \"\"\"Hey there customer, \\\nthe warranty does not cover \\\ncleaning expenses for your kitchen \\\nbecause it's your fault that \\\nyou misused your blender \\\nby forgetting to put the lid on before \\\nstarting the blender. \\\nTough luck! See ya!\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:10:04.19717Z","iopub.execute_input":"2024-04-14T13:10:04.197535Z","iopub.status.idle":"2024-04-14T13:10:04.203463Z","shell.execute_reply.started":"2024-04-14T13:10:04.197507Z","shell.execute_reply":"2024-04-14T13:10:04.201937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"But let‚Äôs say this is what a customer service agent wants. We are going to specify that the service message is going to be translated to this pirate style. So we want it to be in a polite tone that speaks in English pirate. And because we previously created that prompt template, the cool thing is, that we can now reuse that prompt template and specify that the output style we want is this service style pirate and the text is this service reply.\n\n","metadata":{}},{"cell_type":"code","source":"service_style_pirate = \"\"\"\\\na polite tone \\\nthat speaks in English Pirate\\\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:10:45.598636Z","iopub.execute_input":"2024-04-14T13:10:45.599075Z","iopub.status.idle":"2024-04-14T13:10:45.604808Z","shell.execute_reply.started":"2024-04-14T13:10:45.599043Z","shell.execute_reply":"2024-04-14T13:10:45.603374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And if we do that, that‚Äôs the prompt.","metadata":{}},{"cell_type":"code","source":"service_messages = prompt_template.format_messages(\n    style=service_style_pirate,\n    text=service_reply)\n\nprint(service_messages[0].content)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:11:00.895744Z","iopub.execute_input":"2024-04-14T13:11:00.896162Z","iopub.status.idle":"2024-04-14T13:11:00.903386Z","shell.execute_reply.started":"2024-04-14T13:11:00.896132Z","shell.execute_reply":"2024-04-14T13:11:00.901561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And if we prompt, ChatGPT, this is the response it gives us back","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:11:17.65467Z","iopub.execute_input":"2024-04-14T13:11:17.655138Z","iopub.status.idle":"2024-04-14T13:11:17.662657Z","shell.execute_reply.started":"2024-04-14T13:11:17.655105Z","shell.execute_reply":"2024-04-14T13:11:17.661146Z"}}},{"cell_type":"code","source":"service_response = chat(service_messages)\nprint(service_response.content)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:11:24.130853Z","iopub.execute_input":"2024-04-14T13:11:24.131226Z","iopub.status.idle":"2024-04-14T13:11:25.603692Z","shell.execute_reply.started":"2024-04-14T13:11:24.131199Z","shell.execute_reply":"2024-04-14T13:11:25.602752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Why do We Need LangChain Prompt Templates? </b></div>\n\n\n\nWe are using prompt templates instead of just an f-string prompt because as you build sophisticated applications, prompts can be quite long and detailed. Prompt templates are a useful abstraction to help you reuse good prompts when you can.\n\nThis is an example of a relatively long prompt to decide if a student‚Äôs solution is correct or not. And a prompt like this can be quite long, in which you can ask the LLM to first solve the problem, and then have the output in a certain format, and the output in a certain format.","metadata":{}},{"cell_type":"code","source":"prompt = f\"\"\"\nDetermine if the student's solution is correct or not.\n\nQuestion:\nI'm building a solar power installation and I need \\\n help working out the financials. \n- Land costs $100 / square foot\n- I can buy solar panels for $250 / square foot\n- I negotiated a contract for maintenance that will cost \\ \nme a flat $100k per year, and an additional $10 / square \\\nfoot\nWhat is the total cost for the first year of operations \nas a function of the number of square feet.\n\nStudent's Solution:\nLet x be the size of the installation in square feet.\nCosts:\n1. Land cost: 100x\n2. Solar panel cost: 250x\n3. Maintenance cost: 100,000 + 100x\nTotal cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:16:56.708618Z","iopub.execute_input":"2024-04-14T13:16:56.7091Z","iopub.status.idle":"2024-04-14T13:16:56.716143Z","shell.execute_reply.started":"2024-04-14T13:16:56.709066Z","shell.execute_reply":"2024-04-14T13:16:56.714652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LangChain prompt makes it easier to reuse a prompt like this. Also, as you will see in the next articles LangChain provides prompts for some common operations, such as summarization, question answering, connecting to SQL databases, or connecting to different APIs. So by using some of LangChain‚Äôs built-in prompts, you can quickly get an application working without needing to, engineer your prompts.","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ‡ºº‚Å† ‚Å†„Å§‚Å† ‚Å†‚óï‚Å†‚Äø‚Å†‚óï‚Å† ‚Å†‡ºΩ‚Å†„Å§ Thank You!</b></div>\n\n<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> üíå Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> üöÄ If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ‚ù§Ô∏è Once again, thank you for your support, and I hope to see you again soon!</p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}